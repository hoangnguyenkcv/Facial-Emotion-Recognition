{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import argparse\n",
    "import errno\n",
    "import scipy.misc\n",
    "import dlib\n",
    "import cv2\n",
    "\n",
    "from skimage.feature import hog\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from parameters import DATASET, NETWORK\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 expressions\n"
     ]
    }
   ],
   "source": [
    "# initialization\n",
    "image_height = 48\n",
    "image_width = 48\n",
    "window_size = 24\n",
    "window_step = 6\n",
    "ONE_HOT_ENCODING = True\n",
    "SAVE_IMAGES = False\n",
    "GET_LANDMARKS = True\n",
    "GET_HOG_FEATURES = True\n",
    "GET_HOG_IMAGES = True\n",
    "GET_HOG_WINDOWS_FEATURES = True\n",
    "SELECTED_LABELS = []\n",
    "\n",
    "expressions = [0,1,2,3,4,5,6]\n",
    "for i in range(0,len(expressions)):\n",
    "    label = int(expressions[i])\n",
    "    if (label >=0 and label<=6 ):\n",
    "        SELECTED_LABELS.append(label)\n",
    "        \n",
    "if SELECTED_LABELS == []:\n",
    "    SELECTED_LABELS = [0,1,2,3,4,5,6]\n",
    "print( str(len(SELECTED_LABELS)) + \" expressions\")\n",
    "\n",
    "IMAGES_PER_LABEL = 500\n",
    "OUTPUT_FOLDER_NAME = \"fer2013\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing\n",
      "importing csv file\n",
      "converting set: Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hoanganh\\Anaconda3\\lib\\site-packages\\skimage\\feature\\_hog.py:150: skimage_deprecation: Default value of `block_norm`==`L1` is deprecated and will be changed to `L2-Hys` in v0.15. To supress this message specify explicitly the normalization method.\n",
      "  skimage_deprecation)\n",
      "C:\\Users\\hoanganh\\Anaconda3\\lib\\site-packages\\skimage\\feature\\_hog.py:248: skimage_deprecation: Argument `visualise` is deprecated and will be changed to `visualize` in v0.16\n",
      "  'be changed to `visualize` in v0.16', skimage_deprecation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting set: PublicTest...\n",
      "converting set: PrivateTest...\n"
     ]
    }
   ],
   "source": [
    "# loading Dlib predictor and preparing arrays:\n",
    "print( \"preparing\")\n",
    "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n",
    "original_labels = [0, 1, 2, 3, 4, 5, 6]\n",
    "new_labels = list(set(original_labels) & set(SELECTED_LABELS))\n",
    "nb_images_per_label = list(np.zeros(len(new_labels), 'uint8'))\n",
    "\n",
    "try:\n",
    "    os.makedirs(OUTPUT_FOLDER_NAME)\n",
    "except OSError as e:\n",
    "    if e.errno == errno.EEXIST and os.path.isdir(OUTPUT_FOLDER_NAME):\n",
    "        pass\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "def get_landmarks(image, rects):\n",
    "    # this function have been copied from http://bit.ly/2cj7Fpq\n",
    "    if len(rects) > 1:\n",
    "        raise BaseException(\"TooManyFaces\")\n",
    "    if len(rects) == 0:\n",
    "        raise BaseException(\"NoFaces\")\n",
    "    return np.matrix([[p.x, p.y] for p in predictor(image, rects[0]).parts()])\n",
    "\n",
    "def get_new_label(label, one_hot_encoding=False):\n",
    "    if one_hot_encoding:\n",
    "        new_label = new_labels.index(label)\n",
    "        label = list(np.zeros(len(new_labels), 'uint8'))\n",
    "        label[new_label] = 1\n",
    "        return label\n",
    "    else:\n",
    "        return new_labels.index(label)\n",
    "\n",
    "def sliding_hog_windows(image):\n",
    "    hog_windows = []\n",
    "    for y in range(0, image_height, window_step):\n",
    "        for x in range(0, image_width, window_step):\n",
    "            window = image[y:y+window_size, x:x+window_size]\n",
    "            hog_windows.extend(hog(window, orientations=8, pixels_per_cell=(8, 8),\n",
    "                                            cells_per_block=(1, 1), visualise=False))\n",
    "    return hog_windows\n",
    "\n",
    "print( \"importing csv file\")\n",
    "data = pd.read_csv('fer2013.csv')\n",
    "\n",
    "for category in data['Usage'].unique():\n",
    "    print( \"converting set: \" + category + \"...\")\n",
    "    # create folder\n",
    "    if not os.path.exists(category):\n",
    "        try:\n",
    "            os.makedirs(OUTPUT_FOLDER_NAME + '\\\\' + category)\n",
    "        except OSError as e:\n",
    "            if e.errno == errno.EEXIST and os.path.isdir(OUTPUT_FOLDER_NAME):\n",
    "                pass\n",
    "            else:\n",
    "                raise\n",
    "    \n",
    "    # get samples and labels of the actual category\n",
    "    category_data = data[data['Usage'] == category]\n",
    "    samples = category_data['pixels'].values\n",
    "    labels = category_data['emotion'].values\n",
    "    \n",
    "    # get images and extract features\n",
    "    images = []\n",
    "    labels_list = []\n",
    "    landmarks = []\n",
    "    hog_features = []\n",
    "    hog_images = []\n",
    "    for i in range(len(samples)):\n",
    "        try:\n",
    "            if labels[i] in SELECTED_LABELS and nb_images_per_label[get_new_label(labels[i])] < IMAGES_PER_LABEL:\n",
    "\n",
    "                image = np.fromstring(samples[i], dtype=int, sep=\" \").reshape((image_height, image_width))\n",
    "                images.append(image)\n",
    "                if SAVE_IMAGES:\n",
    "\n",
    "                    cv2.imwrite('.\\\\{}\\\\{}.jpg'.format(category,i), image) \n",
    "                    print('.\\\\{}\\\\{}.jpg'.format(category,i))\n",
    "                    \n",
    "                if GET_HOG_WINDOWS_FEATURES:\n",
    "                    features = sliding_hog_windows(image)\n",
    "                    f, hog_image = hog(image, orientations=8, pixels_per_cell=(16, 16),\n",
    "                                            cells_per_block=(1, 1), visualise=True)\n",
    "                    hog_features.append(features)\n",
    "                    if GET_HOG_IMAGES:\n",
    "                        hog_images.append(hog_image)\n",
    "                elif GET_HOG_FEATURES:\n",
    "                    features, hog_image = hog(image, orientations=8, pixels_per_cell=(16, 16),\n",
    "                                            cells_per_block=(1, 1), visualise=True)\n",
    "                    hog_features.append(features)\n",
    "                    if GET_HOG_IMAGES:\n",
    "                        hog_images.append(hog_image)\n",
    "                if GET_LANDMARKS:\n",
    "                    cv2.imwrite('temp.jpg', image)\n",
    "                    image2 = cv2.imread('temp.jpg')\n",
    "                    face_rects = [dlib.rectangle(left=1, top=1, right=47, bottom=47)]\n",
    "                    face_landmarks = get_landmarks(image2, face_rects)\n",
    "                    landmarks.append(face_landmarks)            \n",
    "                labels_list.append(get_new_label(labels[i], one_hot_encoding=ONE_HOT_ENCODING))\n",
    "                nb_images_per_label[get_new_label(labels[i])] += 1\n",
    "        except Exception as e:\n",
    "            print( \"error in image: \" + str(i) + \" - \" + str(e))\n",
    "\n",
    "    np.save(OUTPUT_FOLDER_NAME + '\\\\' + category + '\\\\images.npy', images)\n",
    "    if ONE_HOT_ENCODING:\n",
    "        np.save(OUTPUT_FOLDER_NAME + '\\\\' + category + '\\\\labels.npy', labels_list)\n",
    "    else:\n",
    "        np.save(OUTPUT_FOLDER_NAME + '\\\\' + category + '\\\\labels.npy', labels_list)\n",
    "    if GET_LANDMARKS:\n",
    "        np.save(OUTPUT_FOLDER_NAME + '\\\\' + category + '\\\\landmarks.npy', landmarks)\n",
    "    if GET_HOG_FEATURES or GET_HOG_WINDOWS_FEATURES:\n",
    "        np.save(OUTPUT_FOLDER_NAME + '\\\\' + category + '\\\\hog_features.npy', hog_features)\n",
    "        if GET_HOG_IMAGES:\n",
    "            np.save(OUTPUT_FOLDER_NAME + '\\\\' + category + '\\\\hog_images.npy', hog_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############## Load  Data ######################\n",
    "def load_data(validation=False, test=False):\n",
    "    \n",
    "    data_dict = dict()\n",
    "    validation_dict = dict()\n",
    "    test_dict = dict()\n",
    "\n",
    "    if DATASET.name == \"Fer2013\":\n",
    "\n",
    "        # load train set\n",
    "        data_dict['X'] = np.load(DATASET.train_folder + '/images.npy')\n",
    "        data_dict['X'] = data_dict['X'].reshape([-1, NETWORK.input_size, NETWORK.input_size, 1])\n",
    "        if NETWORK.use_landmarks:\n",
    "            data_dict['X2'] = np.load(DATASET.train_folder + '/landmarks.npy')\n",
    "        if NETWORK.use_hog_and_landmarks:\n",
    "            data_dict['X2'] = np.load(DATASET.train_folder + '/landmarks.npy')\n",
    "            data_dict['X2'] = np.array([x.flatten() for x in data_dict['X2']])\n",
    "            data_dict['X2'] = np.concatenate((data_dict['X2'], np.load(DATASET.train_folder + '/hog_features.npy')), axis=1)\n",
    "        data_dict['Y'] = np.load(DATASET.train_folder + '/labels.npy')\n",
    "        if DATASET.trunc_trainset_to > 0:\n",
    "            data_dict['X'] = data_dict['X'][0:DATASET.trunc_trainset_to, :, :]\n",
    "            if NETWORK.use_landmarks and NETWORK.use_hog_and_landmarks:\n",
    "                data_dict['X2'] = data_dict['X2'][0:DATASET.trunc_trainset_to, :]\n",
    "            elif NETWORK.use_landmarks:\n",
    "                data_dict['X2'] = data_dict['X2'][0:DATASET.trunc_trainset_to, :, :]\n",
    "            data_dict['Y'] = data_dict['Y'][0:DATASET.trunc_trainset_to, :]\n",
    "\n",
    "        if validation:\n",
    "            # load validation set\n",
    "            validation_dict['X'] = np.load(DATASET.validation_folder + '/images.npy')\n",
    "            validation_dict['X'] = validation_dict['X'].reshape([-1, NETWORK.input_size, NETWORK.input_size, 1])\n",
    "            if NETWORK.use_landmarks:\n",
    "                validation_dict['X2'] = np.load(DATASET.validation_folder + '/landmarks.npy')\n",
    "            if NETWORK.use_hog_and_landmarks:\n",
    "                validation_dict['X2'] = np.load(DATASET.validation_folder + '/landmarks.npy')\n",
    "                validation_dict['X2'] = np.array([x.flatten() for x in validation_dict['X2']])\n",
    "                validation_dict['X2'] = np.concatenate((validation_dict['X2'], np.load(DATASET.validation_folder + '/hog_features.npy')), axis=1)\n",
    "            validation_dict['Y'] = np.load(DATASET.validation_folder + '/labels.npy')\n",
    "            if DATASET.trunc_validationset_to > 0:\n",
    "                validation_dict['X'] = validation_dict['X'][0:DATASET.trunc_validationset_to, :, :]\n",
    "                if NETWORK.use_landmarks and NETWORK.use_hog_and_landmarks:\n",
    "                    validation_dict['X2'] = validation_dict['X2'][0:DATASET.trunc_validationset_to, :]\n",
    "                elif NETWORK.use_landmarks:\n",
    "                    validation_dict['X2'] = validation_dict['X2'][0:DATASET.trunc_validationset_to, :, :]\n",
    "                validation_dict['Y'] = validation_dict['Y'][0:DATASET.trunc_validationset_to, :]\n",
    "        \n",
    "        if test:\n",
    "            # load test set\n",
    "            test_dict['X'] = np.load(DATASET.test_folder + '/images.npy')\n",
    "            test_dict['X'] = test_dict['X'].reshape([-1, NETWORK.input_size, NETWORK.input_size, 1])\n",
    "            if NETWORK.use_landmarks:\n",
    "                test_dict['X2'] = np.load(DATASET.test_folder + '/landmarks.npy')\n",
    "            if NETWORK.use_hog_and_landmarks:\n",
    "                test_dict['X2'] = np.load(DATASET.test_folder + '/landmarks.npy')\n",
    "                test_dict['X2'] = np.array([x.flatten() for x in test_dict['X2']])\n",
    "                test_dict['X2'] = np.concatenate((test_dict['X2'], np.load(DATASET.test_folder + '/hog_features.npy')), axis=1)\n",
    "            test_dict['Y'] = np.load(DATASET.test_folder + '/labels.npy')\n",
    "            if DATASET.trunc_testset_to > 0:\n",
    "                test_dict['X'] = test_dict['X'][0:DATASET.trunc_testset_to, :, :]\n",
    "                if NETWORK.use_landmarks and NETWORK.use_hog_and_landmarks:\n",
    "                    test_dict['X2'] = test_dict['X2'][0:DATASET.trunc_testset_to, :]\n",
    "                elif NETWORK.use_landmarks:\n",
    "                    test_dict['X2'] = test_dict['X2'][0:DATASET.trunc_testset_to, :, :]\n",
    "                test_dict['Y'] = test_dict['Y'][0:DATASET.trunc_testset_to, :]\n",
    "\n",
    "        if not validation and not test:\n",
    "            return data_dict\n",
    "        elif not test:\n",
    "            return data_dict, validation_dict\n",
    "        else: \n",
    "            return data_dict, validation_dict, test_dict\n",
    "    else:\n",
    "        print( \"Unknown dataset\")\n",
    "        exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3436, 48, 48)\n",
      "(56, 48, 48)\n",
      "(8, 48, 48)\n",
      "(3436, 7)\n"
     ]
    }
   ],
   "source": [
    "######## \n",
    "\n",
    "a = np.load('fer2013/Training/images.npy')\n",
    "print(a.shape)\n",
    "\n",
    "b = np.load('fer2013/PublicTest/images.npy')\n",
    "print(b.shape)\n",
    "\n",
    "c = np.load('fer2013/PrivateTest/images.npy')\n",
    "print(c.shape)\n",
    "\n",
    "d = np.load('fer2013/Training/labels.npy')\n",
    "print(d.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
